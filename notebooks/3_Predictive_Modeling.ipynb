{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6879077",
   "metadata": {},
   "source": [
    "# Premier League 2024-25 Data Analysis - Part 3: Predictive Modeling\n",
    "\n",
    "This notebook focuses on building predictive models using the Premier League 2024-25 dataset to forecast match outcomes, team rankings, and other relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e999d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5045f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# For evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# For feature importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322df379",
   "metadata": {},
   "source": [
    "## 2. Load the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "cleaned_file_path = '../data/pl_2024_25_cleaned.csv'\n",
    "df = pd.read_csv(cleaned_file_path)\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b6fd5",
   "metadata": {},
   "source": [
    "## 3. Define Prediction Tasks\n",
    "\n",
    "We'll focus on the following prediction tasks:\n",
    "\n",
    "1. Match Outcome Prediction: Predict whether a team will win, lose, or draw\n",
    "2. Goals Prediction: Predict the number of goals a team will score\n",
    "3. Final League Position: Predict the final league position of teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b262cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll focus on Match Outcome Prediction\n",
    "# (to be modified based on actual data structure)\n",
    "\n",
    "# # Define the target variable\n",
    "# if 'result' not in df.columns:\n",
    "#     # Create a 'result' column if it doesn't exist\n",
    "#     # Example logic (to be modified based on actual data):\n",
    "#     # df['result'] = np.where(df['home_goals'] > df['away_goals'], 'win',\n",
    "#     #                       np.where(df['home_goals'] < df['away_goals'], 'loss', 'draw'))\n",
    "#     pass\n",
    "\n",
    "# Display the distribution of the target variable\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.countplot(x='result', data=df)\n",
    "# plt.title('Distribution of Match Results')\n",
    "# plt.xlabel('Result')\n",
    "# plt.ylabel('Count')\n",
    "# plt.grid(axis='y', alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de1a33",
   "metadata": {},
   "source": [
    "## 4. Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target variable\n",
    "# (to be modified based on actual data)\n",
    "\n",
    "# # Features that might be relevant for predicting match outcomes\n",
    "# feature_cols = [\n",
    "#     'home_team', 'away_team',\n",
    "#     'home_goals_scored_avg', 'away_goals_scored_avg',\n",
    "#     'home_goals_conceded_avg', 'away_goals_conceded_avg',\n",
    "#     'home_form', 'away_form',\n",
    "#     'home_shots_avg', 'away_shots_avg',\n",
    "#     'home_possession_avg', 'away_possession_avg',\n",
    "#     'home_win_streak', 'away_win_streak'\n",
    "# ]\n",
    "\n",
    "# # Define X (features) and y (target)\n",
    "# X = df[feature_cols]\n",
    "# y = df['result']\n",
    "\n",
    "# # Display the selected features\n",
    "# print(f\"Selected features: {feature_cols}\")\n",
    "# print(f\"Target variable: result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f46178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "# (to be modified based on actual data)\n",
    "\n",
    "# # Example: Create form features based on recent match results\n",
    "# # def calculate_form(team_results, window=5):\n",
    "# #     # Map results to points: win=3, draw=1, loss=0\n",
    "# #     points = [3 if result == 'win' else 1 if result == 'draw' else 0 for result in team_results]\n",
    "# #     # Calculate rolling sum of points (form)\n",
    "# #     form = pd.Series(points).rolling(window=window, min_periods=1).sum()\n",
    "# #     return form.values\n",
    "# # \n",
    "# # # Group by team and calculate form\n",
    "# # for team in df['team'].unique():\n",
    "# #     team_data = df[df['team'] == team].sort_values('date')\n",
    "# #     df.loc[team_data.index, 'form'] = calculate_form(team_data['result'])\n",
    "\n",
    "# # Example: Create performance metrics based on rolling averages\n",
    "# # def calculate_rolling_average(series, window=5):\n",
    "# #     return series.rolling(window=window, min_periods=1).mean().values\n",
    "# # \n",
    "# # # Group by team and calculate rolling averages\n",
    "# # for team in df['team'].unique():\n",
    "# #     team_data = df[df['team'] == team].sort_values('date')\n",
    "# #     df.loc[team_data.index, 'goals_scored_avg'] = calculate_rolling_average(team_data['goals_scored'])\n",
    "# #     df.loc[team_data.index, 'goals_conceded_avg'] = calculate_rolling_average(team_data['goals_conceded'])\n",
    "# #     df.loc[team_data.index, 'shots_avg'] = calculate_rolling_average(team_data['shots'])\n",
    "# #     df.loc[team_data.index, 'possession_avg'] = calculate_rolling_average(team_data['possession'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796adeaa",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87993459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# print(f\"Training set size: {X_train.shape[0]}\")\n",
    "# print(f\"Testing set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797dc647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing pipeline\n",
    "# (to be modified based on actual data)\n",
    "\n",
    "# # Identify numerical and categorical columns\n",
    "# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# # Define preprocessing for numerical and categorical data\n",
    "# numerical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# # Combine preprocessing steps\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer, numerical_cols),\n",
    "#         ('cat', categorical_transformer, categorical_cols)\n",
    "#     ])\n",
    "\n",
    "# # Check the preprocessing steps\n",
    "# print(\"Numerical columns:\", numerical_cols)\n",
    "# print(\"Categorical columns:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db8397",
   "metadata": {},
   "source": [
    "## 6. Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to evaluate\n",
    "# models = {\n",
    "#     'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "#     'Random Forest': RandomForestClassifier(random_state=42),\n",
    "#     'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "#     'SVM': SVC(probability=True, random_state=42),\n",
    "#     'XGBoost': XGBClassifier(random_state=42)\n",
    "# }\n",
    "\n",
    "# # Dictionary to store results\n",
    "# results = {}\n",
    "# cv_results = {}\n",
    "\n",
    "# # Build and evaluate each model\n",
    "# for name, model in models.items():\n",
    "#     # Create a pipeline with preprocessing and the model\n",
    "#     pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                                ('model', model)])\n",
    "    \n",
    "#     # Fit the model\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "    \n",
    "#     # Make predictions\n",
    "#     y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "#     # Evaluate the model\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred, average='weighted')\n",
    "#     recall = recall_score(y_test, y_pred, average='weighted')\n",
    "#     f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "#     # Store results\n",
    "#     results[name] = {\n",
    "#         'accuracy': accuracy,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall,\n",
    "#         'f1_score': f1\n",
    "#     }\n",
    "    \n",
    "#     # Cross-validation\n",
    "#     cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "#     cv_results[name] = {\n",
    "#         'mean_cv_accuracy': cv_scores.mean(),\n",
    "#         'std_cv_accuracy': cv_scores.std()\n",
    "#     }\n",
    "    \n",
    "#     print(f\"\\n{name}:\")\n",
    "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
    "#     print(f\"Precision: {precision:.4f}\")\n",
    "#     print(f\"Recall: {recall:.4f}\")\n",
    "#     print(f\"F1 Score: {f1:.4f}\")\n",
    "#     print(f\"Cross-validation Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "    \n",
    "#     # Confusion Matrix\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=pipeline.classes_, yticklabels=pipeline.classes_)\n",
    "#     plt.title(f'Confusion Matrix - {name}')\n",
    "#     plt.xlabel('Predicted')\n",
    "#     plt.ylabel('Actual')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "# results_df = pd.DataFrame(results).T\n",
    "# \n",
    "# # Plot the results\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# results_df.plot(kind='bar', figsize=(14, 8))\n",
    "# plt.title('Model Performance Comparison')\n",
    "# plt.xlabel('Model')\n",
    "# plt.ylabel('Score')\n",
    "# plt.ylim(0, 1)\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.grid(axis='y', alpha=0.3)\n",
    "# plt.legend(title='Metric')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edd5a2",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05983576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best performing model\n",
    "# best_model_name = results_df['f1_score'].idxmax()\n",
    "# print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "# # Define hyperparameter grids for the best model\n",
    "# param_grids = {\n",
    "#     'Logistic Regression': {\n",
    "#         'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "#         'model__solver': ['liblinear', 'saga']\n",
    "#     },\n",
    "#     'Random Forest': {\n",
    "#         'model__n_estimators': [50, 100, 200],\n",
    "#         'model__max_depth': [None, 10, 20, 30],\n",
    "#         'model__min_samples_split': [2, 5, 10]\n",
    "#     },\n",
    "#     'Gradient Boosting': {\n",
    "#         'model__n_estimators': [50, 100, 200],\n",
    "#         'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "#         'model__max_depth': [3, 5, 7]\n",
    "#     },\n",
    "#     'SVM': {\n",
    "#         'model__C': [0.1, 1, 10],\n",
    "#         'model__kernel': ['linear', 'rbf'],\n",
    "#         'model__gamma': ['scale', 'auto']\n",
    "#     },\n",
    "#     'XGBoost': {\n",
    "#         'model__n_estimators': [50, 100, 200],\n",
    "#         'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "#         'model__max_depth': [3, 5, 7]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Create pipeline with the best model\n",
    "# best_model = models[best_model_name]\n",
    "# best_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                               ('model', best_model)])\n",
    "\n",
    "# # Grid search\n",
    "# grid_search = GridSearchCV(best_pipeline, param_grids[best_model_name], cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# # Update the best model with the best parameters\n",
    "# best_tuned_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb087315",
   "metadata": {},
   "source": [
    "## 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names after preprocessing\n",
    "# (to be modified based on actual data)\n",
    "\n",
    "# def get_feature_names(column_transformer):\n",
    "#     output_features = []\n",
    "#     for name, pipe, features in column_transformer.transformers_:\n",
    "#         if name != 'remainder':\n",
    "#             if hasattr(pipe.named_steps['onehot'], 'get_feature_names_out'):\n",
    "#                 if pipe.named_steps.get('onehot') is not None:\n",
    "#                     cat_features = pipe.named_steps['onehot'].get_feature_names_out(features)\n",
    "#                     output_features.extend(cat_features.tolist())\n",
    "#             else:\n",
    "#                 output_features.extend(features.tolist())\n",
    "#     return output_features\n",
    "\n",
    "# # Extract feature names (for tree-based models)\n",
    "# if 'Random Forest' in results or 'Gradient Boosting' in results or 'XGBoost' in results:\n",
    "#     # Get feature names after preprocessing\n",
    "#     feature_names = get_feature_names(best_tuned_model.named_steps['preprocessor'])\n",
    "    \n",
    "#     # Get feature importance from the model\n",
    "#     if hasattr(best_tuned_model.named_steps['model'], 'feature_importances_'):\n",
    "#         importances = best_tuned_model.named_steps['model'].feature_importances_\n",
    "        \n",
    "#         # Create a DataFrame for visualization\n",
    "#         importance_df = pd.DataFrame({\n",
    "#             'Feature': feature_names,\n",
    "#             'Importance': importances\n",
    "#         }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "#         # Plot feature importance\n",
    "#         plt.figure(figsize=(12, 8))\n",
    "#         sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
    "#         plt.title('Top 15 Feature Importance')\n",
    "#         plt.xlabel('Importance')\n",
    "#         plt.ylabel('Feature')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         # For models without built-in feature importance, use permutation importance\n",
    "#         perm_importance = permutation_importance(best_tuned_model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "#         importances = perm_importance.importances_mean\n",
    "        \n",
    "#         # Create a DataFrame for visualization\n",
    "#         importance_df = pd.DataFrame({\n",
    "#             'Feature': X.columns,\n",
    "#             'Importance': importances\n",
    "#         }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "#         # Plot feature importance\n",
    "#         plt.figure(figsize=(12, 8))\n",
    "#         sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
    "#         plt.title('Top 15 Feature Importance (Permutation)')\n",
    "#         plt.xlabel('Importance')\n",
    "#         plt.ylabel('Feature')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ae75a",
   "metadata": {},
   "source": [
    "## 9. Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddfc1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification models, we can examine how specific features impact predictions\n",
    "# (to be modified based on actual data and model)\n",
    "\n",
    "# # Example: Examining how possession impacts win probability\n",
    "# # if 'possession' in X.columns:\n",
    "# #     # Create a range of possession values\n",
    "# #     possession_range = np.linspace(X['possession'].min(), X['possession'].max(), 100).reshape(-1, 1)\n",
    "# #     \n",
    "# #     # Create a sample data point with average values\n",
    "# #     X_sample = X.mean().to_dict()\n",
    "# #     \n",
    "# #     # Create an array of predictions for different possession values\n",
    "# #     win_probs = []\n",
    "# #     for poss in possession_range:\n",
    "# #         X_sample['possession'] = poss[0]\n",
    "# #         X_pred = pd.DataFrame([X_sample])\n",
    "# #         win_prob = best_tuned_model.predict_proba(X_pred)[0][1]  # Assuming index 1 is 'win'\n",
    "# #         win_probs.append(win_prob)\n",
    "# #     \n",
    "# #     # Plot the relationship\n",
    "# #     plt.figure(figsize=(10, 6))\n",
    "# #     plt.plot(possession_range, win_probs)\n",
    "# #     plt.title('Impact of Possession on Win Probability')\n",
    "# #     plt.xlabel('Possession (%)')\n",
    "# #     plt.ylabel('Win Probability')\n",
    "# #     plt.grid(True, alpha=0.3)\n",
    "# #     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb537f6",
   "metadata": {},
   "source": [
    "## 10. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle for model saving\n",
    "import pickle\n",
    "\n",
    "# # Save the best model\n",
    "# model_filename = f'../models/best_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl'\n",
    "# with open(model_filename, 'wb') as file:\n",
    "#     pickle.dump(best_tuned_model, file)\n",
    "# \n",
    "# print(f\"Model saved as: {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f746f",
   "metadata": {},
   "source": [
    "## 11. Make Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4724d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Make predictions for upcoming matches\n",
    "# (to be modified based on actual data)\n",
    "\n",
    "# # Create a sample of upcoming matches\n",
    "# # upcoming_matches = pd.DataFrame([\n",
    "# #     {\n",
    "# #         'home_team': 'Manchester City',\n",
    "# #         'away_team': 'Liverpool',\n",
    "# #         'home_goals_scored_avg': 2.5,\n",
    "# #         'away_goals_scored_avg': 2.3,\n",
    "# #         'home_goals_conceded_avg': 0.7,\n",
    "# #         'away_goals_conceded_avg': 0.8,\n",
    "# #         'home_form': 13,\n",
    "# #         'away_form': 12,\n",
    "# #         'home_shots_avg': 18.5,\n",
    "# #         'away_shots_avg': 17.2,\n",
    "# #         'home_possession_avg': 65.3,\n",
    "# #         'away_possession_avg': 62.1,\n",
    "# #         'home_win_streak': 3,\n",
    "# #         'away_win_streak': 2,\n",
    "# #     },\n",
    "# #     # Add more upcoming matches here\n",
    "# # ])\n",
    "# # \n",
    "# # # Make predictions\n",
    "# # predictions = best_tuned_model.predict(upcoming_matches)\n",
    "# # prediction_probs = best_tuned_model.predict_proba(upcoming_matches)\n",
    "# # \n",
    "# # # Add predictions to the DataFrame\n",
    "# # upcoming_matches['predicted_result'] = predictions\n",
    "# # for i, class_name in enumerate(best_tuned_model.classes_):\n",
    "# #     upcoming_matches[f'prob_{class_name}'] = prediction_probs[:, i]\n",
    "# # \n",
    "# # # Display predictions\n",
    "# # upcoming_matches[['home_team', 'away_team', 'predicted_result'] + \n",
    "# #                 [f'prob_{class_name}' for class_name in best_tuned_model.classes_]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693c0db",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "In the next steps, we'll:\n",
    "1. Create an interactive dashboard to showcase the model predictions\n",
    "2. Generate a comprehensive report of our findings\n",
    "3. Set up a system for continuous model updates as new match data becomes available"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
